{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader,  Subset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import staintools\n",
    "from statistics import median\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "import copy \n",
    "from types import SimpleNamespace\n",
    "import models.tent as tent\n",
    "from models.delta import DELTA\n",
    "from models.test_utils import LAME\n",
    "import yaml\n",
    "import torch.optim as optim\n",
    "import models.sar as sar\n",
    "from models.sam import SAM\n",
    "import models.tent as TENT1\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_transform(image_path):\n",
    "    image = staintools.read_image(image_path)\n",
    "    image = staintools.LuminosityStandardizer.standardize(image)\n",
    "    normalized_image = stain_norm.transform(image)\n",
    "    im_pil = Image.fromarray(normalized_image.astype('uint8'), 'RGB')  # Convert to PIL image for transforms\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((m_p_s, m_p_s)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    return transform(im_pil)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.dataset = datasets.ImageFolder(root=self.root)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.dataset.imgs[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img_path)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "\n",
    "def setup_tent(model,steps,episodic,METHOD,noaffine=False):\n",
    "    \"\"\"Set up tent adaptation.\n",
    "\n",
    "    Configure the model for training + feature modulation by batch statistics,\n",
    "    collect the parameters for feature modulation by gradient optimization,\n",
    "    set up the optimizer, and then tent the model.\n",
    "    \"\"\"\n",
    "    model = TENT1.configure_model(model,noaffine=noaffine)\n",
    "    params, param_names = TENT1.collect_params(model)\n",
    "    if METHOD==\"SGD\":\n",
    "        optimizer = optim.SGD(params,\n",
    "                        lr=0.00025,\n",
    "                        momentum=0.9\n",
    "                        )\n",
    "    elif METHOD==\"Adam\":\n",
    "        optimizer = optim.Adam(params,\n",
    "                    lr=1e-3,\n",
    "                    betas=(0.9, 0.999),\n",
    "                    weight_decay=0)\n",
    "\n",
    "    tent_model = TENT1.Tent(model, optimizer,\n",
    "                           steps=steps,#cfg.OPTIM.STEPS\n",
    "                           episodic=episodic,\n",
    "                          noaffine=noaffine)#cfg.MODEL.EPISODIC\n",
    "    return tent_model\n",
    "\n",
    "        \n",
    "def eval_TTA(methods,adapt_model,chosen_loader):\n",
    "#     with torch.no_grad():\n",
    "\n",
    "        correct = {}\n",
    "        total = {}\n",
    "        batch_acc = {}\n",
    "        class_num = {}\n",
    "        class_correct = {}\n",
    "        class_avg_acc = {}\n",
    "        cumulative_acc = {}\n",
    "        for method in methods:\n",
    "            correct[method] = 0\n",
    "            total[method] = 0   \n",
    "            batch_acc[method] = 0\n",
    "            class_num[method] = np.array([0]*10)\n",
    "            class_correct[method] = np.array([0]*10)\n",
    "            class_avg_acc[method] = 0\n",
    "            cumulative_acc[method] = 0\n",
    "        results={}\n",
    "        for method in methods:\n",
    "            batch_idx=0\n",
    "            iters = iter(chosen_loader)\n",
    "            print(method)\n",
    "            while(batch_idx<10):\n",
    "                print(batch_idx)\n",
    "                try:\n",
    "                    inputs, targets = next(iters)\n",
    "                except:\n",
    "                    print(\"empty mask\")\n",
    "                    continue\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                batch_result={}\n",
    "                model=adapt_model[method]\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total[method] = targets.size(0)\n",
    "                correct[method] = predicted.eq(targets).sum().item()\n",
    "                batch_acc[method] = 100.*correct[method]/total[method]\n",
    "                for i, t in enumerate(targets):\n",
    "                    class_num[method][t.item()] += 1\n",
    "                    class_correct[method][t.item()] += (predicted[i]==t)\n",
    "                acc = (class_correct[method][class_num[method]!=0] / class_num[method][class_num[method]!=0])\n",
    "                class_avg_acc[method] = acc.mean() * 100.\n",
    "                cumulative_acc[method] = class_correct[method].sum() / class_num[method].sum() * 100\n",
    "                batch_result[f'{method}_cumulative_accuracy_{batch_idx}'] = cumulative_acc[method]\n",
    "                batch_result[f'{method}_batch_accuracy_{batch_idx}'] = batch_acc[method]\n",
    "                batch_result[f'{method}_class_accuracy_{batch_idx}'] = class_avg_acc[method]\n",
    "\n",
    "                results.update(batch_result)\n",
    "                tentnet.reset()\n",
    "                batch_idx+=1\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.cor_path=\"02_training_native\"\n",
    "        self.model_name=\"TvN_350_SN_D256_Initial_Ep7_fullmodel.pth\"\n",
    "        self.exp_type=\"imbalanced_experiments\"\n",
    "        self.batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parser = argparse.ArgumentParser(description='pathology TTA')\n",
    "# parser.add_argument('--artifact', type=str)\n",
    "# parser.add_argument('--cor_path', type=str)\n",
    "# parser.add_argument('--exp_type', default = 'imbalanced_experiments', type=str)\n",
    "# parser.add_argument('--rho', required=False, type=float, help='long tail factor')\n",
    "# parser.add_argument('--pi', required=False, type=float, help='dirichlet factor')\n",
    "# parser.add_argument('--model_name', default = 'TvN_350_SN_D256_Initial_Ep7_fullmodel.pth', type=str)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "parameters={}\n",
    "corrupt_data_path=args.cor_path\n",
    "model_name = args.model_name\n",
    "\n",
    "# print(artifact)\n",
    "# print(os.getcwd())\n",
    "st = staintools.read_image(\"./Artifact/15_stain_scheme/schemes_ready/standard_he_stain_small.jpg\")\n",
    "standardizer = staintools.LuminosityStandardizer.standardize(st)\n",
    "stain_norm = staintools.StainNormalizer(method='macenko')\n",
    "stain_norm.fit(st)\n",
    "number_of_classes=3\n",
    "model_dir = 'Models'\n",
    "m_p_s = 350\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_dist(args,labels):\n",
    "    len_dataset =len(labels)\n",
    "    exp_setup=args.exp_setup\n",
    "    if exp_setup == \"IS+CB\":\n",
    "        idx = [i for i in range(len_dataset)]\n",
    "        random.shuffle(idx)\n",
    "            \n",
    "    elif exp_setup == \"IS+CI\":\n",
    "        idx = dirichlet_split_noniid(np.array(labels), args.pi, 10)\n",
    "        idx = np.concatenate(idx)\n",
    "    elif exp_setup == \"DS+CB\":\n",
    "        \n",
    "        prob_per_class = []\n",
    "        for cls_idx in range(number_of_classes):\n",
    "            prob_per_class.append( args.rho ** (cls_idx / (number_of_classes - 1.0)) )\n",
    "        prob_per_class = np.array(prob_per_class) / sum(prob_per_class)\n",
    "        img_per_class = prob_per_class * len(labels)\n",
    "        idx = []\n",
    "        y_test_np = np.array(labels)\n",
    "        for c, num in enumerate(img_per_class):\n",
    "            all_of_c = np.where(y_test_np==c)[0]\n",
    "            idx.append(np.random.choice(all_of_c, int(num)+1))\n",
    "        idx = np.concatenate(idx)\n",
    "        random.shuffle(idx)\n",
    "        \n",
    "    elif exp_setup == \"DS+CI\":\n",
    "        \n",
    "        prob_per_class = []\n",
    "        for cls_idx in range(number_of_classes):\n",
    "            prob_per_class.append( args.rho ** (cls_idx / (number_of_classes - 1.0)) )\n",
    "        prob_per_class = np.array(prob_per_class) / sum(prob_per_class)\n",
    "        img_per_class = prob_per_class * len(labels)\n",
    "        idx = []\n",
    "        y_test_np = np.array(labels)\n",
    "        for c, num in enumerate(img_per_class):\n",
    "            all_of_c = np.where(y_test_np==c)[0]\n",
    "            idx.append(np.random.choice(all_of_c, int(num)+1))\n",
    "        idx = np.concatenate(idx)\n",
    "        idx2 = dirichlet_split_noniid(np.array([y_test_np[i] for i in idx]), args.pi, 10)\n",
    "        idx = np.concatenate([idx[i] for i in idx2])\n",
    "        \n",
    "    return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments=[\n",
    " {\"exp_setup\":\"IS+CB\"},\n",
    " {\"exp_setup\":\"DS+CB\", \"rho\":1},\n",
    " {\"exp_setup\":\"DS+CB\", \"rho\":0.5},\n",
    " {\"exp_setup\":\"DS+CB\", \"rho\":0.1},\n",
    " {\"exp_setup\":\"IS+CI\", \"pi\":0.1},\n",
    " {\"exp_setup\":\"IS+CI\", \"pi\":0.05},\n",
    " {\"exp_setup\":\"DS+CI\", \"rho\":0.5, \"pi\":0.1},\n",
    " {\"exp_setup\":\"DS+CI\", \"rho\":0.5, \"pi\":0.05},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp:\n",
    "    def __init__(self, exp):\n",
    "        self.exp_setup=exp[\"exp_setup\"]\n",
    "        self.rho=exp[\"rho\"] if \"rho\" in exp.keys() else None\n",
    "        self.pi=exp[\"pi\"] if \"pi\" in exp.keys() else None\n",
    "        self.setup_name=f\"{self.exp_setup}\"\n",
    "        if self.rho:\n",
    "            self.setup_name+=f\"_rho{self.rho}\"\n",
    "        if self.pi:\n",
    "            self.setup_name+=f\"_pi{self.pi}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= f\"TTA_on_corrupted/{args.exp_type}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "empty mask\n",
      "3\n",
      "empty mask\n",
      "3\n",
      "empty mask\n",
      "3\n",
      "empty mask\n",
      "3\n",
      "empty mask\n",
      "3\n",
      "empty mask\n",
      "3\n",
      "empty mask\n",
      "3\n",
      "empty mask\n",
      "3\n",
      "empty mask\n",
      "3\n",
      "empty mask\n",
      "3\n",
      "empty mask\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for exp in experiments:\n",
    "    experiment_args=Exp(exp)\n",
    "    setup=\"\"\n",
    "    parameters=exp\n",
    "    json_data=[]\n",
    "    for seed in  [0,19,22, 42, 81]: #2020 , 42, 81\n",
    "        parameters[\"seed\"]=seed\n",
    "        parameters[\"model\"]=args.model_name\n",
    "        \n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        with open('configs/cifar_tentdelta_adam.yaml', 'rb') as f:\n",
    "            args_tent = yaml.safe_load(f.read())\n",
    "        config_obj = SimpleNamespace(**args_tent)\n",
    "\n",
    "        path_model = os.path.join(model_dir, model_name)\n",
    "        net = torch.load(path_model) \n",
    "        net = net.cuda()\n",
    "        net.eval()\n",
    "        lame_model=LAME(copy.deepcopy(net),3,5,1)\n",
    "        delta_model= DELTA(config_obj,copy.deepcopy(net))\n",
    "        sarnet = sar.configure_model(copy.deepcopy(net))\n",
    "        params, param_names = sar.collect_params(sarnet)\n",
    "        base_optimizer = torch.optim.SGD\n",
    "        optimizer = SAM(params, base_optimizer, lr=0.00025, momentum=0.9) #lr suitable for batch size >32\n",
    "        sar_model = sar.SAR(sarnet, optimizer, margin_e0=math.log(3)*0.40) # since we have 3 classes\n",
    "\n",
    "        tentnet = setup_tent(copy.deepcopy(net),10,False,METHOD=\"Adam\")\n",
    "        TENT1.check_model(tentnet)\n",
    "\n",
    "\n",
    "        methods=[\"TENT\", \"DELTA\",\"NOT_ADAPTED\",\"SAR\",\"LAME\"] #\"TTN\" \"DELTA\",\"NOT_ADAPTED\", \"LAME\", \"DELTA\",\"NOT_ADAPTED\", \"LAME\",\"SAR\"\n",
    "        adapt_model={\"DELTA\":delta_model,\"TENT\":tentnet,\"NOT_ADAPTED\":net,\"LAME\":lame_model,\"SAR\":sar_model} #\"TTN\":norm_net_TTN, \n",
    "        custom_dataset = CustomDataset(root=f\"Corrupted_data/{corrupt_data_path}/00_original\", transform=custom_transform)\n",
    "        labels = custom_dataset.dataset.targets\n",
    "        idx = sample_data_dist(experiment_args,labels)\n",
    "        subset = Subset(custom_dataset, idx)\n",
    "        dataloader = DataLoader(subset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "        correct = {}\n",
    "        total = {}\n",
    "        batch_acc = {}\n",
    "        class_num = {}\n",
    "        class_correct = {}\n",
    "        class_avg_acc = {}\n",
    "        cumulative_acc = {}\n",
    "        for method in methods:\n",
    "            correct[method] = 0\n",
    "            total[method] = 0   \n",
    "            batch_acc[method] = 0\n",
    "            class_num[method] = np.array([0]*3)\n",
    "            class_correct[method] = np.array([0]*3)\n",
    "            class_avg_acc[method] = 0\n",
    "            cumulative_acc[method] = 0\n",
    "        results={}\n",
    "        for method in methods:\n",
    "            batch_idx=0\n",
    "            iters = iter(dataloader)\n",
    "            while(batch_idx<10):\n",
    "                print(batch_idx)\n",
    "                try:\n",
    "                    inputs, targets = next(iters)\n",
    "                except:\n",
    "                    print(\"empty mask\")\n",
    "                    continue\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                batch_result={}\n",
    "                outputs=adapt_model[method](inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total[method] = targets.size(0)\n",
    "                correct[method] = predicted.eq(targets).sum().item()\n",
    "                batch_acc[method] = 100.*correct[method]/total[method]\n",
    "                for i, t in enumerate(targets):\n",
    "                    class_num[method][t.item()] += 1\n",
    "                    class_correct[method][t.item()] += (predicted[i]==t)\n",
    "                acc = (class_correct[method][class_num[method]!=0] / class_num[method][class_num[method]!=0])\n",
    "                class_avg_acc[method] = acc.mean() * 100.\n",
    "                cumulative_acc[method] = class_correct[method].sum() / class_num[method].sum() * 100\n",
    "                batch_result[f'{method}_cumulative_accuracy_{batch_idx}'] = cumulative_acc[method]\n",
    "                batch_result[f'{method}_batch_accuracy_{batch_idx}'] = batch_acc[method]\n",
    "                batch_result[f'{method}_class_accuracy_{batch_idx}'] = class_avg_acc[method]\n",
    "\n",
    "                results.update(batch_result)\n",
    "                adapt_model[\"TENT\"].reset()\n",
    "                batch_idx+=1\n",
    "\n",
    "        json_entry = {\"parameters\": parameters,\n",
    "                \"results\": results\n",
    "                }\n",
    "        json_data.append(json_entry)\n",
    "\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    with open(f\"{path}/results_TTA_{setup}_{model_name[:-4]}.json\", 'w') as json_file:\n",
    "        json.dump(json_data, json_file, indent=4, separators=(',',': '))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (pathology)",
   "language": "python",
   "name": "pathology"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
