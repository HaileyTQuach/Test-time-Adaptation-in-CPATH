{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EdiSlyKREtkC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import staintools\n",
    "from statistics import median\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_transform(image_path):\n",
    "    image = staintools.read_image(image_path)\n",
    "#     print(image)\n",
    "    image = staintools.LuminosityStandardizer.standardize(image)\n",
    "    normalized_image = stain_norm.transform(image)\n",
    "    im_pil = Image.fromarray(normalized_image.astype('uint8'), 'RGB')  # Convert to PIL image for transforms\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((m_p_s, m_p_s)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    return transform(im_pil)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.dataset = datasets.ImageFolder(root=self.root)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.dataset.imgs[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img_path)\n",
    "        return img, label\n",
    "    \n",
    "\n",
    "def eval(model,dataloader):\n",
    "    accuracies=[]\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in tqdm(enumerate(dataloader)):\n",
    "        if batch_idx==10:\n",
    "            break\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "        total_correct += torch.sum(predicted == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "        accuracy = total_correct / total_samples\n",
    "        accuracies.append(accuracy)\n",
    "    return accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTA methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import utils.thresholding_adaptingspecificchannels as thc_specific\n",
    "# import utils.thresholding_combined as thc\n",
    "import copy \n",
    "from types import SimpleNamespace\n",
    "# import models.tent as tent\n",
    "from models.delta import DELTA\n",
    "from models.test_utils import LAME\n",
    "import yaml\n",
    "import torch.optim as optim\n",
    "import models.sar as sar\n",
    "from models.sam import SAM\n",
    "import models.tent as TENT1\n",
    "# import models.tent_2 as TENT2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tent(model,steps,episodic,noaffine=False):\n",
    "    \"\"\"Set up tent adaptation.\n",
    "\n",
    "    Configure the model for training + feature modulation by batch statistics,\n",
    "    collect the parameters for feature modulation by gradient optimization,\n",
    "    set up the optimizer, and then tent the model.\n",
    "    \"\"\"\n",
    "    model = TENT1.configure_model(model,noaffine=noaffine)\n",
    "    params, param_names = TENT1.collect_params(model)\n",
    "    optimizer = setup_tent_optimizer(params,METHOD=\"SGD\",LR=0.00025, momentum=0.9)\n",
    "    tent_model = TENT1.Tent(model, optimizer,\n",
    "                           steps=steps,#cfg.OPTIM.STEPS\n",
    "                           episodic=episodic,\n",
    "                          noaffine=noaffine)#cfg.MODEL.EPISODIC\n",
    "    return tent_model\n",
    "\n",
    "def setup_tent_optimizer(params,METHOD=\"Adam\",WD=0,LR = 1e-3,BETA = 0.9,momentum=0.9):\n",
    "    \"\"\"Set up optimizer for tent adaptation.\n",
    "\n",
    "    Tent needs an optimizer for test-time entropy minimization.\n",
    "    In principle, tent could make use of any gradient optimizer.\n",
    "    In practice, we advise choosing Adam or SGD+momentum.\n",
    "    For optimization settings, we advise to use the settings from the end of\n",
    "    trainig, if known, or start with a low learning rate (like 0.001) if not.\n",
    "\n",
    "    For best results, try tuning the learning rate and batch size.\n",
    "    \"\"\"\n",
    "    if METHOD == 'Adam':\n",
    "        return optim.Adam(params,\n",
    "                    lr=LR,\n",
    "                    betas=(BETA, 0.999),\n",
    "                    weight_decay=WD)\n",
    "    elif METHOD == 'SGD':\n",
    "        return optim.SGD(params,\n",
    "                        lr=LR,\n",
    "                        momentum=0.9\n",
    "                        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "def eval_TTA(methods,adapt_model,chosen_loader):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct = {}\n",
    "        total = {}\n",
    "        batch_acc = {}\n",
    "        class_num = {}\n",
    "        class_correct = {}\n",
    "        class_avg_acc = {}\n",
    "        cumulative_acc = {}\n",
    "        for method in methods:\n",
    "            correct[method] = 0\n",
    "            total[method] = 0   \n",
    "            batch_acc[method] = 0\n",
    "            class_num[method] = np.array([0]*10)\n",
    "            class_correct[method] = np.array([0]*10)\n",
    "            class_avg_acc[method] = 0\n",
    "            cumulative_acc[method] = 0\n",
    "        results={}\n",
    "        for method in methods:\n",
    "            for batch_idx, (inputs, targets) in tqdm(enumerate(chosen_loader)):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                batch_result={}\n",
    "                if batch_idx==10:\n",
    "                    break\n",
    "                model=adapt_model[method]\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total[method] = targets.size(0)\n",
    "                correct[method] = predicted.eq(targets).sum().item()\n",
    "                batch_acc[method] = 100.*correct[method]/total[method]\n",
    "                for i, t in enumerate(targets):\n",
    "                    class_num[method][t.item()] += 1\n",
    "                    class_correct[method][t.item()] += (predicted[i]==t)\n",
    "                acc = (class_correct[method][class_num[method]!=0] / class_num[method][class_num[method]!=0])\n",
    "                class_avg_acc[method] = acc.mean() * 100.\n",
    "                cumulative_acc[method] = class_correct[method].sum() / class_num[method].sum() * 100\n",
    "                batch_result[f'{method}_cumulative_accuracy_{batch_idx}'] = cumulative_acc[method]\n",
    "                batch_result[f'{method}_batch_accuracy_{batch_idx}'] = batch_acc[method]\n",
    "                batch_result[f'{method}_class_accuracy_{batch_idx}'] = class_avg_acc[method]\n",
    "\n",
    "                results.update(batch_result)\n",
    "                tentnet.reset()\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'Models'\n",
    "model_names = sorted(os.listdir(model_dir))\n",
    "m_p_s = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact=\"01_focus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_data_path=\"Corrupted_data/02_training_native/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = staintools.read_image(\"./Artifact/15_stain_scheme/schemes_ready/standard_he_stain_small.jpg\")\n",
    "standardizer = staintools.LuminosityStandardizer.standardize(st)\n",
    "stain_norm = staintools.StainNormalizer(method='macenko')\n",
    "stain_norm.fit(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "model name: TvN_350_SN_D256_Initial_Ep7_fullmodel.pth artifact:  01_focus\n",
      "TENT\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "DELTA\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "NOT_ADAPTED\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "LAME\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SAR\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "model name: TvN_350_SN_D256_v2_Ep1_fullmodel.pth artifact:  01_focus\n",
      "TENT\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "DELTA\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "NOT_ADAPTED\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/p/paria.mehrbod/.conda/envs/pathology/lib/python3.9/site-packages/staintools/stain_normalizer.py:41: RuntimeWarning: divide by zero encountered in divide\n",
      "  source_concentrations *= (self.maxC_target / maxC_source)\n",
      "/home/mila/p/paria.mehrbod/.conda/envs/pathology/lib/python3.9/site-packages/staintools/stain_normalizer.py:41: RuntimeWarning: invalid value encountered in multiply\n",
      "  source_concentrations *= (self.maxC_target / maxC_source)\n",
      "/home/mila/p/paria.mehrbod/.conda/envs/pathology/lib/python3.9/site-packages/staintools/stain_normalizer.py:43: RuntimeWarning: invalid value encountered in cast\n",
      "  return tmp.reshape(I.shape).astype(np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "LAME\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SAR\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "model name: TvN_350_SN_D256_Initial_Ep7_fullmodel.pth artifact:  01_focus\n",
      "TENT\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "DELTA\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "NOT_ADAPTED\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "LAME\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SAR\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "model name: TvN_350_SN_D256_v2_Ep1_fullmodel.pth artifact:  01_focus\n",
      "TENT\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "DELTA\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "NOT_ADAPTED\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "LAME\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SAR\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "model name: TvN_350_SN_D256_Initial_Ep7_fullmodel.pth artifact:  01_focus\n",
      "TENT\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "DELTA\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "NOT_ADAPTED\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "LAME\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SAR\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "model name: TvN_350_SN_D256_v2_Ep1_fullmodel.pth artifact:  01_focus\n",
      "TENT\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "DELTA\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "NOT_ADAPTED\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "LAME\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SAR\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "model name: TvN_350_SN_D256_Initial_Ep7_fullmodel.pth artifact:  01_focus\n",
      "TENT\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "DELTA\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "NOT_ADAPTED\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "LAME\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SAR\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "model name: TvN_350_SN_D256_v2_Ep1_fullmodel.pth artifact:  01_focus\n",
      "TENT\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "DELTA\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "NOT_ADAPTED\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "LAME\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SAR\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "model name: TvN_350_SN_D256_Initial_Ep7_fullmodel.pth artifact:  01_focus\n",
      "TENT\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "DELTA\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "NOT_ADAPTED\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "LAME\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SAR\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "model name: TvN_350_SN_D256_v2_Ep1_fullmodel.pth artifact:  01_focus\n",
      "TENT\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "DELTA\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "NOT_ADAPTED\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "LAME\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "SAR\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for seed in  [0,19,22, 42, 81]: \n",
    "    json_data=[]\n",
    "    for model_name in model_names:\n",
    "        with open('configs/cifar_tentdelta.yaml', 'rb') as f:\n",
    "            args_tent = yaml.safe_load(f.read())\n",
    "        config_obj = SimpleNamespace(**args_tent)\n",
    "\n",
    "        path_model = os.path.join(model_dir, model_name)\n",
    "        net = torch.load(path_model) \n",
    "        net = net.cuda()\n",
    "        net.eval()\n",
    "\n",
    "        lame_model=LAME(copy.deepcopy(net),10,5,1)\n",
    "        delta_model= DELTA(config_obj,copy.deepcopy(net))\n",
    "        \n",
    "        net = sar.configure_model(net)\n",
    "        params, param_names = sar.collect_params(net)\n",
    "        base_optimizer = torch.optim.SGD\n",
    "        optimizer = SAM(params, base_optimizer, lr=0.00025, momentum=0.9) #lr suitable for batch size >32\n",
    "        sar_model = sar.SAR(net, optimizer, margin_e0=math.log(1000)*0.40)\n",
    "\n",
    "        tentnet = setup_tent(copy.deepcopy(net),1,False)\n",
    "        TENT1.check_model(tentnet)\n",
    "        \n",
    "#         net = TENT2.configure_model(net)\n",
    "#         params, param_names = TENT2.collect_params(net)\n",
    "#         optimizer = torch.optim.SGD(params,0.00025, momentum=0.9) \n",
    "#         sar_tent_model = TENT2.Tent(net, optimizer)\n",
    "\n",
    "        \n",
    "        methods=[\"TENT\",\"DELTA\",\"NOT_ADAPTED\", \"LAME\",\"SAR\"] #\"TTN\" \"DELTA\",\"NOT_ADAPTED\", \"LAME\",\n",
    "        adapt_model={\"DELTA\":delta_model,\"TENT\":tentnet,\"NOT_ADAPTED\":net,\"LAME\":lame_model,\"SAR\":sar_model} #\"TTN\":norm_net_TTN, \"SAR_TENT\":sar_tent_model\n",
    "        print(\"model name:\", model_name, \"artifact: \",artifact)\n",
    "        custom_dataset = CustomDataset(root=f\"{corrupt_data_path}/{artifact}/\", transform=custom_transform)\n",
    "        chosen_loader = DataLoader(custom_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            correct = {}\n",
    "            total = {}\n",
    "            batch_acc = {}\n",
    "            class_num = {}\n",
    "            class_correct = {}\n",
    "            class_avg_acc = {}\n",
    "            cumulative_acc = {}\n",
    "            for method in methods:\n",
    "                correct[method] = 0\n",
    "                total[method] = 0   \n",
    "                batch_acc[method] = 0\n",
    "                class_num[method] = np.array([0]*10)\n",
    "                class_correct[method] = np.array([0]*10)\n",
    "                class_avg_acc[method] = 0\n",
    "                cumulative_acc[method] = 0\n",
    "            results={}\n",
    "            \n",
    "            for method in methods:\n",
    "                iters = iter(chosen_loader)\n",
    "                batch_idx=0\n",
    "                print(method)\n",
    "                while(batch_idx<=10):\n",
    "                    try:\n",
    "                        inputs, targets = next(iters)\n",
    "                    except:\n",
    "                        continue\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    batch_result={}\n",
    "                    if batch_idx==10:\n",
    "                        break\n",
    "                    model=adapt_model[method]\n",
    "                    outputs = model(inputs)\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total[method] = targets.size(0)\n",
    "                    correct[method] = predicted.eq(targets).sum().item()\n",
    "                    batch_acc[method] = 100.*correct[method]/total[method]\n",
    "                    for i, t in enumerate(targets):\n",
    "                        class_num[method][t.item()] += 1\n",
    "                        class_correct[method][t.item()] += (predicted[i]==t)\n",
    "                    acc = (class_correct[method][class_num[method]!=0] / class_num[method][class_num[method]!=0])\n",
    "                    class_avg_acc[method] = acc.mean() * 100.\n",
    "                    cumulative_acc[method] = class_correct[method].sum() / class_num[method].sum() * 100\n",
    "                    batch_result[f'{method}_cumulative_accuracy_{batch_idx}'] = cumulative_acc[method]\n",
    "                    batch_result[f'{method}_batch_accuracy_{batch_idx}'] = batch_acc[method]\n",
    "                    batch_result[f'{method}_class_accuracy_{batch_idx}'] = class_avg_acc[method]\n",
    "\n",
    "                    results.update(batch_result)\n",
    "                    tentnet.reset()\n",
    "                    batch_idx+=1\n",
    "                    print(batch_idx)\n",
    "        \n",
    "        \n",
    "        \n",
    "        json_entry = {\n",
    "            'parameters': {\n",
    "            \"seed\":seed,\n",
    "            \"artifact\": artifact,\n",
    "            \"model\" : model_name\n",
    "            },\n",
    "            'results': results\n",
    "            }\n",
    "        json_data.append(json_entry)\n",
    "    with open(f\"results_testsar_TTA_{artifact}_{model_name}.json\", 'w') as json_file:\n",
    "        json.dump(json_data, json_file, indent=4, separators=(',',': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"results_testsar_TTA_{artifact}_{model_name}.json\", 'w') as json_file:\n",
    "        json.dump(json_data, json_file, indent=4, separators=(',',': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (pathology)",
   "language": "python",
   "name": "pathology"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
