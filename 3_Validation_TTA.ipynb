{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EdiSlyKREtkC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import staintools\n",
    "from statistics import median\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_transform(image_path):\n",
    "    image = staintools.read_image(image_path)\n",
    "#     print(image)\n",
    "    image = staintools.LuminosityStandardizer.standardize(image)\n",
    "    normalized_image = stain_norm.transform(image)\n",
    "    im_pil = Image.fromarray(normalized_image.astype('uint8'), 'RGB')  # Convert to PIL image for transforms\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((m_p_s, m_p_s)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    return transform(im_pil)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.dataset = datasets.ImageFolder(root=self.root)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.dataset.imgs[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img_path)\n",
    "        return img, label\n",
    "    \n",
    "\n",
    "def eval(model,dataloader):\n",
    "    accuracies=[]\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in tqdm(enumerate(dataloader)):\n",
    "        if batch_idx==10:\n",
    "            break\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "        total_correct += torch.sum(predicted == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "        accuracy = total_correct / total_samples\n",
    "        accuracies.append(accuracy)\n",
    "    return accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTA methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import utils.thresholding_adaptingspecificchannels as thc_specific\n",
    "# import utils.thresholding_combined as thc\n",
    "import copy \n",
    "from types import SimpleNamespace\n",
    "import models.tent as tent\n",
    "from models.delta import DELTA\n",
    "from models.test_utils import LAME\n",
    "import yaml\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tent(model,steps,episodic,noaffine=False):\n",
    "    \"\"\"Set up tent adaptation.\n",
    "\n",
    "    Configure the model for training + feature modulation by batch statistics,\n",
    "    collect the parameters for feature modulation by gradient optimization,\n",
    "    set up the optimizer, and then tent the model.\n",
    "    \"\"\"\n",
    "    model = tent.configure_model(model,noaffine=noaffine)\n",
    "    params, param_names = tent.collect_params(model)\n",
    "    optimizer = setup_tent_optimizer(params)\n",
    "    tent_model = tent.Tent(model, optimizer,\n",
    "                           steps=steps,#cfg.OPTIM.STEPS\n",
    "                           episodic=episodic,\n",
    "                          noaffine=noaffine)#cfg.MODEL.EPISODIC\n",
    "    return tent_model\n",
    "\n",
    "def setup_tent_optimizer(params,METHOD=\"Adam\",WD=0,LR = 1e-3,BETA = 0.9):\n",
    "    \"\"\"Set up optimizer for tent adaptation.\n",
    "\n",
    "    Tent needs an optimizer for test-time entropy minimization.\n",
    "    In principle, tent could make use of any gradient optimizer.\n",
    "    In practice, we advise choosing Adam or SGD+momentum.\n",
    "    For optimization settings, we advise to use the settings from the end of\n",
    "    trainig, if known, or start with a low learning rate (like 0.001) if not.\n",
    "\n",
    "    For best results, try tuning the learning rate and batch size.\n",
    "    \"\"\"\n",
    "    if METHOD == 'Adam':\n",
    "        return optim.Adam(params,\n",
    "                    lr=LR,\n",
    "                    betas=(BETA, 0.999),\n",
    "                    weight_decay=WD)\n",
    "    elif METHOD == 'SGD':\n",
    "        return None\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "def eval_TTA(methods,adapt_model,chosen_loader):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        correct = {}\n",
    "        total = {}\n",
    "        batch_acc = {}\n",
    "        class_num = {}\n",
    "        class_correct = {}\n",
    "        class_avg_acc = {}\n",
    "        cumulative_acc = {}\n",
    "        for method in methods:\n",
    "            correct[method] = 0\n",
    "            total[method] = 0   \n",
    "            batch_acc[method] = 0\n",
    "            class_num[method] = np.array([0]*10)\n",
    "            class_correct[method] = np.array([0]*10)\n",
    "            class_avg_acc[method] = 0\n",
    "            cumulative_acc[method] = 0\n",
    "        results={}\n",
    "        for method in methods:\n",
    "            for batch_idx, (inputs, targets) in tqdm(enumerate(chosen_loader)):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                batch_result={}\n",
    "                if batch_idx==10:\n",
    "                    break\n",
    "                model=adapt_model[method]\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total[method] = targets.size(0)\n",
    "                correct[method] = predicted.eq(targets).sum().item()\n",
    "                batch_acc[method] = 100.*correct[method]/total[method]\n",
    "                for i, t in enumerate(targets):\n",
    "                    class_num[method][t.item()] += 1\n",
    "                    class_correct[method][t.item()] += (predicted[i]==t)\n",
    "                acc = (class_correct[method][class_num[method]!=0] / class_num[method][class_num[method]!=0])\n",
    "                class_avg_acc[method] = acc.mean() * 100.\n",
    "                cumulative_acc[method] = class_correct[method].sum() / class_num[method].sum() * 100\n",
    "                batch_result[f'{method}_cumulative_accuracy_{batch_idx}'] = cumulative_acc[method]\n",
    "                batch_result[f'{method}_batch_accuracy_{batch_idx}'] = batch_acc[method]\n",
    "                batch_result[f'{method}_class_accuracy_{batch_idx}'] = class_avg_acc[method]\n",
    "\n",
    "                results.update(batch_result)\n",
    "                tentnet.reset()\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'Models'\n",
    "model_names = sorted(os.listdir(model_dir))\n",
    "m_p_s = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact=\"01_focus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_data_path=\"Corrupted_data/02_training_native/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = staintools.read_image(\"./Artifact/15_stain_scheme/schemes_ready/standard_he_stain_small.jpg\")\n",
    "standardizer = staintools.LuminosityStandardizer.standardize(st)\n",
    "stain_norm = staintools.StainNormalizer(method='macenko')\n",
    "stain_norm.fit(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "model name: TvN_350_SN_D256_Initial_Ep7_fullmodel.pth artifact:  01_focus\n",
      "DELTA\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "NOT_ADAPTED\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "TENT\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "LAME\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "model name: TvN_350_SN_D256_v2_Ep1_fullmodel.pth artifact:  01_focus\n",
      "DELTA\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "NOT_ADAPTED\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "TENT\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "LAME\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m     json_data\u001b[38;5;241m.\u001b[39mappend(json_entry)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults_TTA_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00martifact\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[0;32m---> 89\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mdump(json_data, json_file, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, separators\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "for seed in  [0,19,22, 42, 81]: \n",
    "    json_data=[]\n",
    "    for model_name in model_names:\n",
    "        with open('configs/cifar_tentdelta.yaml', 'rb') as f:\n",
    "            args_tent = yaml.safe_load(f.read())\n",
    "        config_obj = SimpleNamespace(**args_tent)\n",
    "\n",
    "        path_model = os.path.join(model_dir, model_name)\n",
    "        net = torch.load(path_model) \n",
    "        net = net.cuda()\n",
    "        net.eval()\n",
    "\n",
    "        lame_model=LAME(copy.deepcopy(net),10,5,1)\n",
    "        delta_model= DELTA(config_obj,copy.deepcopy(net))\n",
    "        tentnet = setup_tent(copy.deepcopy(net),1,False)\n",
    "        tent.check_model(tentnet)    \n",
    "        methods=[\"DELTA\",\"NOT_ADAPTED\",\"TENT\",\"LAME\"] #\"TTN\"\n",
    "        adapt_model={\"DELTA\":delta_model,\"TENT\":tentnet,\"NOT_ADAPTED\":net,\"LAME\":lame_model} #\"TTN\":norm_net_TTN\n",
    "        print(\"model name:\", model_name, \"artifact: \",artifact)\n",
    "        custom_dataset = CustomDataset(root=f\"{corrupt_data_path}/{artifact}/\", transform=custom_transform)\n",
    "        chosen_loader = DataLoader(custom_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            correct = {}\n",
    "            total = {}\n",
    "            batch_acc = {}\n",
    "            class_num = {}\n",
    "            class_correct = {}\n",
    "            class_avg_acc = {}\n",
    "            cumulative_acc = {}\n",
    "            for method in methods:\n",
    "                correct[method] = 0\n",
    "                total[method] = 0   \n",
    "                batch_acc[method] = 0\n",
    "                class_num[method] = np.array([0]*10)\n",
    "                class_correct[method] = np.array([0]*10)\n",
    "                class_avg_acc[method] = 0\n",
    "                cumulative_acc[method] = 0\n",
    "            results={}\n",
    "            \n",
    "            for method in methods:\n",
    "                iters = iter(chosen_loader)\n",
    "                batch_idx=0\n",
    "                print(method)\n",
    "                while(batch_idx<=10):\n",
    "                    try:\n",
    "                        inputs, targets = next(iters)\n",
    "                    except:\n",
    "                        print(\"AAAAAAAAAAAA\")\n",
    "                        continue\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    batch_result={}\n",
    "                    if batch_idx==10:\n",
    "                        break\n",
    "                    model=adapt_model[method]\n",
    "                    outputs = model(inputs)\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total[method] = targets.size(0)\n",
    "                    correct[method] = predicted.eq(targets).sum().item()\n",
    "                    batch_acc[method] = 100.*correct[method]/total[method]\n",
    "                    for i, t in enumerate(targets):\n",
    "                        class_num[method][t.item()] += 1\n",
    "                        class_correct[method][t.item()] += (predicted[i]==t)\n",
    "                    acc = (class_correct[method][class_num[method]!=0] / class_num[method][class_num[method]!=0])\n",
    "                    class_avg_acc[method] = acc.mean() * 100.\n",
    "                    cumulative_acc[method] = class_correct[method].sum() / class_num[method].sum() * 100\n",
    "                    batch_result[f'{method}_cumulative_accuracy_{batch_idx}'] = cumulative_acc[method]\n",
    "                    batch_result[f'{method}_batch_accuracy_{batch_idx}'] = batch_acc[method]\n",
    "                    batch_result[f'{method}_class_accuracy_{batch_idx}'] = class_avg_acc[method]\n",
    "\n",
    "                    results.update(batch_result)\n",
    "                    tentnet.reset()\n",
    "                    batch_idx+=1\n",
    "                    print(batch_idx)\n",
    "        \n",
    "        \n",
    "        \n",
    "        json_entry = {\n",
    "            'parameters': {\n",
    "            \"seed\":seed,\n",
    "            \"artifact\": artifact,\n",
    "            \"model\" : model_name\n",
    "            },\n",
    "            'results': results\n",
    "            }\n",
    "        json_data.append(json_entry)\n",
    "    with open(f\"results_TTA_{artifact}_{model_name}.json\", 'w') as json_file:\n",
    "        json.dump(json_data, json_file, indent=4, separators=(',',': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (pathology)",
   "language": "python",
   "name": "pathology"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
