{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader,  Subset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import staintools\n",
    "from statistics import median\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "import copy \n",
    "from types import SimpleNamespace\n",
    "import models.tent as tent\n",
    "from models.delta import DELTA\n",
    "from models.test_utils import LAME\n",
    "import yaml\n",
    "import torch.optim as optim\n",
    "import models.sar as sar\n",
    "from models.sam import SAM\n",
    "import models.tent as TENT1\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_transform(image_path):\n",
    "    image = staintools.read_image(image_path)\n",
    "    image = staintools.LuminosityStandardizer.standardize(image)\n",
    "    normalized_image = stain_norm.transform(image)\n",
    "    im_pil = Image.fromarray(normalized_image.astype('uint8'), 'RGB')  # Convert to PIL image for transforms\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize((m_p_s, m_p_s)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    return transform(im_pil)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.dataset = datasets.ImageFolder(root=self.root)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.dataset.imgs[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img_path)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "\n",
    "def setup_tent(model,steps,episodic,METHOD,noaffine=False):\n",
    "    \"\"\"Set up tent adaptation.\n",
    "\n",
    "    Configure the model for training + feature modulation by batch statistics,\n",
    "    collect the parameters for feature modulation by gradient optimization,\n",
    "    set up the optimizer, and then tent the model.\n",
    "    \"\"\"\n",
    "    model = TENT1.configure_model(model,noaffine=noaffine)\n",
    "    params, param_names = TENT1.collect_params(model)\n",
    "    if METHOD==\"SGD\":\n",
    "        optimizer = optim.SGD(params,\n",
    "                        lr=0.00025,\n",
    "                        momentum=0.9\n",
    "                        )\n",
    "    elif METHOD==\"Adam\":\n",
    "        optimizer = optim.Adam(params,\n",
    "                    lr=1e-3,\n",
    "                    betas=(0.9, 0.999),\n",
    "                    weight_decay=0)\n",
    "\n",
    "    tent_model = TENT1.Tent(model, optimizer,\n",
    "                           steps=steps,#cfg.OPTIM.STEPS\n",
    "                           episodic=episodic,\n",
    "                          noaffine=noaffine)#cfg.MODEL.EPISODIC\n",
    "    return tent_model\n",
    "\n",
    "        \n",
    "def eval_TTA(methods,adapt_model,chosen_loader):\n",
    "#     with torch.no_grad():\n",
    "\n",
    "        correct = {}\n",
    "        total = {}\n",
    "        batch_acc = {}\n",
    "        class_num = {}\n",
    "        class_correct = {}\n",
    "        class_avg_acc = {}\n",
    "        cumulative_acc = {}\n",
    "        for method in methods:\n",
    "            correct[method] = 0\n",
    "            total[method] = 0   \n",
    "            batch_acc[method] = 0\n",
    "            class_num[method] = np.array([0]*10)\n",
    "            class_correct[method] = np.array([0]*10)\n",
    "            class_avg_acc[method] = 0\n",
    "            cumulative_acc[method] = 0\n",
    "        results={}\n",
    "        for method in methods:\n",
    "            batch_idx=0\n",
    "            iters = iter(chosen_loader)\n",
    "            print(method)\n",
    "            while(batch_idx<10):\n",
    "                print(batch_idx)\n",
    "                try:\n",
    "                    inputs, targets = next(iters)\n",
    "                except:\n",
    "                    print(\"empty mask\")\n",
    "                    continue\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                batch_result={}\n",
    "                model=adapt_model[method]\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total[method] = targets.size(0)\n",
    "                correct[method] = predicted.eq(targets).sum().item()\n",
    "                batch_acc[method] = 100.*correct[method]/total[method]\n",
    "                for i, t in enumerate(targets):\n",
    "                    class_num[method][t.item()] += 1\n",
    "                    class_correct[method][t.item()] += (predicted[i]==t)\n",
    "                acc = (class_correct[method][class_num[method]!=0] / class_num[method][class_num[method]!=0])\n",
    "                class_avg_acc[method] = acc.mean() * 100.\n",
    "                cumulative_acc[method] = class_correct[method].sum() / class_num[method].sum() * 100\n",
    "                batch_result[f'{method}_cumulative_accuracy_{batch_idx}'] = cumulative_acc[method]\n",
    "                batch_result[f'{method}_batch_accuracy_{batch_idx}'] = batch_acc[method]\n",
    "                batch_result[f'{method}_class_accuracy_{batch_idx}'] = class_avg_acc[method]\n",
    "\n",
    "                results.update(batch_result)\n",
    "                tentnet.reset()\n",
    "                batch_idx+=1\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.artifact=\"00_original\"\n",
    "        self.model_name=\"TvN_350_SN_D256_Initial_Ep7_fullmodel.pth\"\n",
    "        self.batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parser = argparse.ArgumentParser(description='pathology TTA')\n",
    "# parser.add_argument('--artifact', type=str)\n",
    "# parser.add_argument('--model_name', default = 'TvN_350_SN_D256_Initial_Ep7_fullmodel.pth', type=str)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "st = staintools.read_image(\"./Artifact/15_stain_scheme/schemes_ready/standard_he_stain_small.jpg\")\n",
    "standardizer = staintools.LuminosityStandardizer.standardize(st)\n",
    "stain_norm = staintools.StainNormalizer(method='macenko')\n",
    "stain_norm.fit(st)\n",
    "\n",
    "model_dir = 'Models'\n",
    "m_p_s = 350\n",
    "model_name = args.model_name\n",
    "artifact =\"00_original\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/p/paria.mehrbod/.conda/envs/pathology/lib/python3.9/site-packages/staintools/stain_normalizer.py:41: RuntimeWarning: divide by zero encountered in divide\n",
      "  source_concentrations *= (self.maxC_target / maxC_source)\n",
      "/home/mila/p/paria.mehrbod/.conda/envs/pathology/lib/python3.9/site-packages/staintools/stain_normalizer.py:41: RuntimeWarning: invalid value encountered in multiply\n",
      "  source_concentrations *= (self.maxC_target / maxC_source)\n",
      "/home/mila/p/paria.mehrbod/.conda/envs/pathology/lib/python3.9/site-packages/staintools/stain_normalizer.py:43: RuntimeWarning: invalid value encountered in cast\n",
      "  return tmp.reshape(I.shape).astype(np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "here\n",
      "#Trainable/total parameters: 0/24033347 \t Fraction: 0.00% \n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dataset_name in [\"01_case_western_native\",\"03_wns_leica_native\",\"04_wns_hama_native\",\"05_wns_glis_native\",\"06_ukk_native\"]:\n",
    "    path= f\"TTA_on_corrupted/second_dataset_{dataset_name}\"\n",
    "    parameters={}\n",
    "    parameters[\"artifact\"]=args.artifact\n",
    "    parameters[\"model\"]=args.model_name\n",
    "    parameters[\"dataset_name\"]=dataset_name\n",
    "    json_data=[]\n",
    "    for seed in  [0,19,22, 42, 81]: #2020 , 42, 81\n",
    "        parameters[\"seed\"]=seed\n",
    "\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        with open('configs/cifar_tentdelta_adam.yaml', 'rb') as f:\n",
    "            args_tent = yaml.safe_load(f.read())\n",
    "        config_obj = SimpleNamespace(**args_tent)\n",
    "\n",
    "        path_model = os.path.join(model_dir, model_name)\n",
    "        net = torch.load(path_model) \n",
    "        net = net.cuda()\n",
    "        net.eval()\n",
    "        lame_model=LAME(copy.deepcopy(net),3,5,1)\n",
    "        delta_model= DELTA(config_obj,copy.deepcopy(net))\n",
    "        sarnet = sar.configure_model(copy.deepcopy(net))\n",
    "        params, param_names = sar.collect_params(sarnet)\n",
    "        base_optimizer = torch.optim.SGD\n",
    "        optimizer = SAM(params, base_optimizer, lr=0.00025, momentum=0.9) #lr suitable for batch size >32\n",
    "        sar_model = sar.SAR(sarnet, optimizer, margin_e0=math.log(3)*0.40) # since we have 3 classes\n",
    "\n",
    "        tentnet = setup_tent(copy.deepcopy(net),10,False,METHOD=\"Adam\")\n",
    "        TENT1.check_model(tentnet)\n",
    "\n",
    "\n",
    "        methods=[\"TENT\", \"DELTA\",\"NOT_ADAPTED\",\"SAR\",\"LAME\"] #\"TTN\" \"DELTA\",\"NOT_ADAPTED\", \"LAME\", \"DELTA\",\"NOT_ADAPTED\", \"LAME\",\"SAR\"\n",
    "        adapt_model={\"DELTA\":delta_model,\"TENT\":tentnet,\"NOT_ADAPTED\":net,\"LAME\":lame_model,\"SAR\":sar_model} #\"TTN\":norm_net_TTN, \n",
    "        custom_dataset = CustomDataset(root=f\"Training_data/{dataset_name}/\", transform=custom_transform)\n",
    "        dataloader = DataLoader(custom_dataset, batch_size=64, shuffle=True)    \n",
    "    \n",
    "        correct = {}\n",
    "        total = {}\n",
    "        batch_acc = {}\n",
    "        class_num = {}\n",
    "        class_correct = {}\n",
    "        class_avg_acc = {}\n",
    "        cumulative_acc = {}\n",
    "        for method in methods:\n",
    "            correct[method] = 0\n",
    "            total[method] = 0   \n",
    "            batch_acc[method] = 0\n",
    "            class_num[method] = np.array([0]*3)\n",
    "            class_correct[method] = np.array([0]*3)\n",
    "            class_avg_acc[method] = 0\n",
    "            cumulative_acc[method] = 0\n",
    "        results={}\n",
    "        for method in methods:\n",
    "            batch_idx=0\n",
    "            iters = iter(dataloader)\n",
    "            while(batch_idx<10):\n",
    "                print(batch_idx)\n",
    "                try:\n",
    "                    inputs, targets = next(iters)\n",
    "                except:\n",
    "                    print(\"empty mask\")\n",
    "                    continue\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                batch_result={}\n",
    "                outputs=adapt_model[method](inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total[method] = targets.size(0)\n",
    "                correct[method] = predicted.eq(targets).sum().item()\n",
    "                batch_acc[method] = 100.*correct[method]/total[method]\n",
    "                for i, t in enumerate(targets):\n",
    "                    class_num[method][t.item()] += 1\n",
    "                    class_correct[method][t.item()] += (predicted[i]==t)\n",
    "                acc = (class_correct[method][class_num[method]!=0] / class_num[method][class_num[method]!=0])\n",
    "                class_avg_acc[method] = acc.mean() * 100.\n",
    "                cumulative_acc[method] = class_correct[method].sum() / class_num[method].sum() * 100\n",
    "                batch_result[f'{method}_cumulative_accuracy_{batch_idx}'] = cumulative_acc[method]\n",
    "                batch_result[f'{method}_batch_accuracy_{batch_idx}'] = batch_acc[method]\n",
    "                batch_result[f'{method}_class_accuracy_{batch_idx}'] = class_avg_acc[method]\n",
    "\n",
    "                results.update(batch_result)\n",
    "                adapt_model[\"TENT\"].reset()\n",
    "                batch_idx+=1\n",
    "\n",
    "        json_entry = {\"parameters\": parameters,\n",
    "                \"results\": results\n",
    "                }\n",
    "        json_data.append(json_entry)\n",
    "\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    with open(f\"{path}/results_TTA_{artifact}_{model_name[:-4]}.json\", 'w') as json_file:\n",
    "        json.dump(json_data, json_file, indent=4, separators=(',',': '))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (pathology)",
   "language": "python",
   "name": "pathology"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
